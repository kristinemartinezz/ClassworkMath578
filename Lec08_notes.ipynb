{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read an overview of this Numerical Linear Algebra course in [this blog post](http://www.fast.ai/2017/07/17/num-lin-alg/).  The course was originally taught in the [University of San Francisco MS in Analytics](https://www.usfca.edu/arts-sciences/graduate-programs/analytics) graduate program.  Course lecture videos are [available on YouTube](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY) (note that the notebook numbers and video numbers do not line up, since some notebooks took longer than 1 video to cover).\n",
    "\n",
    "You can ask questions about the course on [our fast.ai forums](http://forums.fast.ai/c/lin-alg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. PageRank with Eigen Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Handy Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two tools that we'll be using today, which are useful in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [Psutil](https://github.com/giampaolo/psutil) is a great way to check on your memory usage.  This will be useful here since we are using a larger data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmem(rss=86306816, vms=90599424, num_page_faults=22937, peak_wset=87257088, wset=86306816, peak_paged_pool=290808, paged_pool=290616, peak_nonpaged_pool=120052, nonpaged_pool=108440, pagefile=90599424, peak_pagefile=91783168, private=90599424)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "t = process.memory_info()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90599424, 86306816)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vms, t.rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mem_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / psutil.virtual_memory().total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005082681204077709"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [TQDM](https://github.com/tqdm/tqdm) gives you progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Without TQDM\n",
    "s = 0\n",
    "for i in range(10):\n",
    "    s += i\n",
    "    sleep(0.2)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# With TQDM\n",
    "from tqdm import tqdm\n",
    "\n",
    "s = 0\n",
    "for i in tqdm(range(10)):\n",
    "    s += i\n",
    "    sleep(0.2)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review**\n",
    "- What is SVD?\n",
    "- What are some applications of SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional SVD Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting use of SVD that I recently came across was as a step in the de-biasing of Word2Vec word embeddings, from [Quantifying and Reducing Stereotypes in Word Embeddings](https://arxiv.org/pdf/1606.06121.pdf)(Bolukbasi, et al).\n",
    "\n",
    "Word2Vec is a useful library released by Google that represents words as vectors.  The similarity of vectors captures semantic meaning, and analogies can be found, such as *Paris:France :: Tokyo: Japan*.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/word2vec_analogies.png\" alt=\"\" style=\"width: 80%\"/>\n",
    "(source: [Vector Representations of Words](https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/))\n",
    "\n",
    "However, these embeddings can implicitly encode bias, such as *father:doctor :: mother:nurse* and *man:computer programmer :: woman:homemaker*.\n",
    "\n",
    "One approach for de-biasing the space involves using SVD to reduce the dimensionality ([Bolukbasi paper](https://arxiv.org/pdf/1606.06121.pdf)).\n",
    "\n",
    "You can read more about bias in word embeddings:\n",
    "- [How Vector Space Mathematics Reveals the Hidden Sexism in Language](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/)(MIT Tech Review)\n",
    "- [ConceptNet: better, less-stereotyped word vectors](https://blog.conceptnet.io/2017/04/24/conceptnet-numberbatch-17-04-better-less-stereotyped-word-vectors/)\n",
    "- [Semantics derived automatically from language corpora necessarily contain human biases](https://www.princeton.edu/~aylinc/papers/caliskan-islam_semantics.pdf) (excellent and very interesting paper!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways to think about SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data compression\n",
    "- SVD trades a large number of features for a smaller set of better features\n",
    "- All matrices are diagonal (if you use change of bases on the domain and range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspectives on SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually talk about SVD in terms of matrices, $$A = U \\Sigma V^T$$ but we can also think about it in terms of vectors.  SVD gives us sets of orthonormal vectors ${v_j}$ and ${u_j}$ such that $$ A v_j = \\sigma_j u_j $$\n",
    "\n",
    "$\\sigma_j$ are scalars, called singular values\n",
    "\n",
    "**Q**: Does this remind you of anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship between SVD and Eigen Decomposition**: the left-singular vectors of A are the eigenvectors of $AA^T$. The right-singular vectors of A are the eigenvectors of $A^T A$. The non-zero singular values of A are the square roots of the eigenvalues of $A^T A$ (and $A A^T$).\n",
    "\n",
    "SVD is a generalization of eigen decomposition. Not all matrices have eigen values, but ALL matrices have singular values.\n",
    "\n",
    "Let's forget SVD for a bit and talk about how to find the eigenvalues of a symmetric positive definite matrix..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources on SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [SVD: image compression and least squares](http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/)\n",
    "\n",
    "- [Image Compression with SVD](http://nbviewer.jupyter.org/gist/frankcleary/4d2bd178708503b556b0)\n",
    "\n",
    "- [The Extraordinary SVD](https://sites.math.washington.edu/~morrow/498_13/svd_applied.pdf)\n",
    "\n",
    "- [A Singularly Valuable Decomposition: The SVD of a Matrix](https://sites.math.washington.edu/~morrow/498_13/svd.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today: Eigen Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best classical methods for computing the SVD are variants on methods for computing eigenvalues.  In addition to their links to SVD, Eigen decompositions are useful on their own as well.  Here are a few practical applications of eigen decomposition:\n",
    "- [rapid matrix powers](http://www.onmyphd.com/?p=eigen.decomposition#h2_why)\n",
    "- [nth Fibonacci number](http://mathproofs.blogspot.com/2005/04/nth-term-of-fibonacci-sequence.html)\n",
    "- Behavior of ODEs\n",
    "- Markov Chains (health care economics, Page Rank)\n",
    "- [Linear Discriminant Analysis on Iris dataset](http://sebastianraschka.com/Articles/2014_python_lda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the 3 Blue 1 Brown videos on [Change of basis](https://www.youtube.com/watch?v=P2LTAUO1TdA) and [Eigenvalues and eigenvectors](https://www.youtube.com/watch?v=PFDu9oVAE-g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Eigenvalues are a way to see into the heart of a matrix... All the difficulties of matrices are swept away\" -Strang "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocab**: A **Hermitian** matrix is one that is equal to it's own conjugate transpose.  In the case of real-valued matrices (which is all we are considering in this course), **Hermitian** means the same as **Symmetric**.\n",
    "\n",
    "**Relevant Theorems:**\n",
    "- If A is symmetric, then eigenvalues of A are real and $A = Q \\Lambda Q^T$\n",
    "- If A is triangular, then its eigenvalues are equal to its diagonal entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBpedia Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the **Power Method**, which finds one eigenvector.  *What good is just one eigenvector?* you may be wondering.  This is actually the basis for PageRank (read [The $25,000,000,000 Eigenvector: the Linear Algebra Behind Google](http://www.rose-hulman.edu/~bryan/googleFinalVersionFixed.pdf) for more info)\n",
    "\n",
    "Instead of trying to rank the importance of all websites on the internet, we are going to use a dataset of Wikipedia links from [DBpedia](http://wiki.dbpedia.org/).  DBpedia provides structured Wikipedia data available in 125 languages.  \n",
    "\n",
    "\"*The full DBpedia data set features 38 million labels and abstracts in 125 different languages, 25.2 million links to images and 29.8 million links to external web pages; 80.9 million links to Wikipedia categories, and 41.2 million links to [YAGO](http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/) categories*\" --[about DBpedia](http://wiki.dbpedia.org/about)\n",
    "\n",
    "Today's lesson is inspired by this [SciKit Learn Example](http://scikit-learn.org/stable/auto_examples/applications/wikipedia_principal_eigenvector.html#sphx-glr-auto-examples-applications-wikipedia-principal-eigenvector-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pickle\n",
    "from bz2 import BZ2File\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.decomposition import randomized_svd\n",
    "from joblib import memory\n",
    "#from sklearn.externals.joblib import Memory\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have are:\n",
    "- **redirects**: URLs that redirect to other URLs\n",
    "- **links**: which pages link to which other pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'redirects_en.nt.bz2', please wait...\n",
      "Downloading 'page_links_en.nt.bz2', please wait...\n"
     ]
    }
   ],
   "source": [
    "PATH = 'data/dbpedia/'\n",
    "URL_BASE = 'http://downloads.dbpedia.org/3.5.1/en/'\n",
    "filenames = [\"redirects_en.nt.bz2\", \"page_links_en.nt.bz2\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    if not os.path.exists(PATH+filename):\n",
    "        print(\"Downloading '%s', please wait...\" % filename)\n",
    "        open(PATH+filename, 'wb').write(urlopen(URL_BASE+filename).read()) #.read() downloads information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "redirects_filename = PATH+filenames[0]\n",
    "page_links_filename = PATH+filenames[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Adjacency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a graph **adjacency matrix**, of which pages point to which.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/graph.png\" alt=\"\" style=\"width: 25%\"/>\n",
    "(source: [PageRank and HyperLink Induced Topic Search](https://www.slideshare.net/priyabrata232/page-rank-and-hyperlink))\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/adjaceny_matrix.png\" alt=\"\" style=\"width: 80%\"/>\n",
    "(source: [PageRank and HyperLink Induced Topic Search](https://www.slideshare.net/priyabrata232/page-rank-and-hyperlink))\n",
    "\n",
    "The power $A^2$ will give you how many ways there are to get from one page to another in 2 steps.  You can see a more detailed example, as applied to airline travel, [worked out in these notes](http://www.utdallas.edu/~jwz120030/Teaching/PastCoursesUMBC/M221HS06/ProjectFiles/Adjacency.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep track of which pages point to which pages.  We will store this in a  square matrix, with a $1$ in position $(r, c)$ indicating that the topic in row $r$ points to the topic in column $c$\n",
    "\n",
    "You can read [more about graphs here](http://www.geeksforgeeks.org/graph-and-its-representations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One line of the file looks like:\n",
    "- `<http://dbpedia.org/resource/AfghanistanHistory> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/History_of_Afghanistan> .`\n",
    "\n",
    "In the below slice, the plus 1, -1 are to remove the <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "DBPEDIA_RESOURCE_PREFIX_LEN = len(\"http://dbpedia.org/resource/\")\n",
    "SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_lines(filename): return (line.split() for line in BZ2File(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through redirections and create dictionary of source to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_redirect(targ, redirects):\n",
    "    seen = set()\n",
    "    while True:\n",
    "        transitive_targ = targ\n",
    "        targ = redirects.get(targ)\n",
    "        if targ is None or targ in seen: break\n",
    "        seen.add(targ)\n",
    "    return transitive_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_redirects(redirects_filename):\n",
    "    redirects={}\n",
    "    lines = get_lines(redirects_filename)\n",
    "    return {src[SLICE]:get_redirect(targ[SLICE], redirects) \n",
    "                for src,_,targ,_ in tqdm_notebook(lines, leave=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects = get_redirects(redirects_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_item(lst, redirects, index_map, item):\n",
    "    k = item[SLICE]\n",
    "    lst.append(index_map.setdefault(redirects.get(k, k), len(index_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "limit=119077682 #5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the integer index map\n",
    "index_map = dict() # links->IDs\n",
    "lines = get_lines(page_links_filename)\n",
    "source, destination, data = [],[],[]\n",
    "for l, split in tqdm_notebook(enumerate(lines), total=limit):\n",
    "    if l >= limit: break\n",
    "    add_item(source, redirects, index_map, split[0])\n",
    "    add_item(destination, redirects, index_map, split[2])\n",
    "    data.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(data); n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below steps are just to illustrate what info is in our data and how it is structured.  They are not efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of items are in index_map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map.popitem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one item in our index map:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1940_Cincinnati_Reds_Team_Issue has index $9991173$.  This only shows up once in the destination list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,x in enumerate(source) if x == 9991173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[119077649], destination[119077649]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check which page is the source (has index $9991050$).  Note: usually you should not access a dictionary by searching for its values.  This is inefficient and not how dictionaries are intended to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_name, index in index_map.items():\n",
    "    if index == 9991050:\n",
    "        print(page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on Wikipedia that the Cincinati Red Teams Issue has [redirected to W711-2](https://en.wikipedia.org/wiki/W711-2):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/cincinnati_reds.png\" alt=\"\" style=\"width: 70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_inds = [i for i,x in enumerate(source) if x == 9991050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_dests = [destination[i] for i in test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check which page is the source (has index 9991174):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_name, index in index_map.items():\n",
    "    if index in test_dests:\n",
    "        print(page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the items in the list appear in the wikipedia page:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/cincinnati_reds2.png\" alt=\"\" style=\"width: 100%\"/>\n",
    "(Source: [Wikipedia](https://en.wikipedia.org/wiki/W711-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a sparse matrix using Scipy's COO format, and that convert it to CSR.\n",
    "\n",
    "**Questions**: What are COO and CSR?  Why would we create it with COO and then convert it right away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = sparse.coo_matrix((data, (destination,source)), shape=(n,n), dtype=np.float32)\n",
    "X = X.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "del(data,destination, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save matrix so we don't have to recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open(PATH+'X.pkl', 'wb'))\n",
    "pickle.dump(index_map, open(PATH+'index_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open(PATH+'X.pkl', 'rb'))\n",
    "index_map = pickle.load(open(PATH+'index_map.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $n \\times n$ matrix $A$ is **diagonalizable** if it has $n$ linearly independent eigenvectors $v_1,\\, \\ldots v_n$.\n",
    "\n",
    "Then any $w$ can be expressed $w = \\sum_{j=1}^n c_j v_j $, for some scalars $c_j$.\n",
    "\n",
    "**Exercise:** Show that $$ A^k w = \\sum_{j=1}^n c_j \\lambda_j^k v_j$$\n",
    "\n",
    "**Question**: How will this behave for large $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inspiration for the **power method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_ex(v):\n",
    "    print(', '.join(names[i].decode() for i in np.abs(v.squeeze()).argsort()[-1:-10:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "?np.squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to normalize a sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sparse.csr_matrix(np.array([[1,2],[3,4]]))\n",
    "Sr = S.sum(axis=0).A1; Sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[numpy.matrix.A1](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.A1.html#numpy.matrix.A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.data / np.take(Sr, S.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def power_method(A, max_iter=100):\n",
    "    n = A.shape[1]\n",
    "    A = np.copy(A)\n",
    "    A.data /= np.take(A.sum(axis=0).A1, A.indices)\n",
    "\n",
    "    scores = np.ones(n, dtype=np.float32) * np.sqrt(A.sum()/(n*n)) # initial guess\n",
    "    for i in range(max_iter):\n",
    "        scores = A @ scores\n",
    "        nrm = np.linalg.norm(scores)\n",
    "        scores /= nrm\n",
    "        print(nrm)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why normalize the scores on each iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = power_method(X, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many advanced eigenvalue algorithms that are used in practice are variations on the power method.\n",
    "\n",
    "In Lesson 3: Background Removal, we used Facebook's [fast randomized pca/svd library, fbpca](https://github.com/facebook/fbpca).  Check out [the source code](https://github.com/facebook/fbpca/blob/master/fbpca.py#L1549) for the pca method we used. It uses the power method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Study**\n",
    "\n",
    "- Check out [Google Page Rank, Power Iteration and the Second EigenValue of the Google Matrix](http://rstudio-pubs-static.s3.amazonaws.com/239261_8a607707294341c4b7e26acf728c28bd.html) for animations of the distribution as it converges.\n",
    "\n",
    "- The convergence rate of the power method is the ratio of the largest eigenvalue to the 2nd largest eigenvalue.  It can be speeded up by adding *shifts*.  To find eigenvalues other than the largest, a method called *deflation* can be used.  See Chapter 12.1 of Greenbaum & Chartier for more details.\n",
    "\n",
    "- *Krylov Subspaces*: In the Power Iteration, notice that we multiply by our matrix A each time, effectively calculating $$Ab,\\,A^2b,\\,A^3b,\\,A^4b\\, \\ldots$$\n",
    "  \n",
    "  The matrix with those vectors as columns is called the *Krylov matrix*, and the space spanned by those vectors is the *Krylov subspace*.  Keep this in mind for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare to SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time U, s, V = randomized_svd(X, 3, n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Top wikipedia pages according to principal singular vectors\n",
    "show_ex(U.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_ex(U.T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_ex(V[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_ex(V[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Exercise:** Normalize the data in various ways.  Don't overwrite the adjacency matrix, but instead create a new one.  See how your results differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Eigen Decomposition vs SVD:**\n",
    "- SVD involves 2 bases, eigen decomposition involves 1 basis\n",
    "- SVD bases are orthonormal, eigen basis generally not orthogonal\n",
    "- All matrices have an SVD, not all matrices (not even all square) have an eigen decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the power method to find the eigenvector corresponding to the largest eigenvalue of our matrix of Wikipedia links.  This eigenvector gave us the relative importance of each Wikipedia page (like a simplified PageRank).\n",
    "\n",
    "Next, let's look at a method for finding all eigenvalues of a symmetric, positive definite matrix.  This method includes 2 fundamental algorithms in numerical linear algebra, and is a basis for many more complex methods.\n",
    "\n",
    "[The Second Eigenvalue of the Google Matrix](https://nlp.stanford.edu/pubs/secondeigenvalue.pdf): has \"implications for the convergence rate of the standard PageRank algorithm as the web scales, for the stability of PageRank to perturbations to the link structure of the web, for the detection of Google spammers, and for the design of algorithms to speed up PageRank\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Confusion: QR Algorithm vs QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **QR algorithm** uses something called the **QR decomposition**.  Both are important, so don't get them confused.  The **QR decomposition** decomposes a matrix $A = QR$ into a set of orthonormal columns $Q$ and a triangular matrix $R$.  We will look at several ways to calculate the QR decomposition in a future lesson.  For now, just know that it is giving us an orthogonal matrix and a triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two matrices $A$ and $B$ are **similar** if there exists a non-singular matrix $X$ such that $$B = X^{-1}AX$$\n",
    "\n",
    "Watch this: [Change of Basis](https://www.youtube.com/watch?v=P2LTAUO1TdA&index=13&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "\n",
    "**Theorem**: If $X$ is non-singular, then $A$ and $X^{-1}AX$ have the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Schur factorization** of a matrix $A$ is a factorization:\n",
    "$$ A = Q T Q^*$$\n",
    "where $Q$ is unitary and $T$ is upper-triangular.\n",
    "\n",
    "**Question**: What can you say about the eigenvalues of $A$?\n",
    "\n",
    "**Theorem:** Every square matrix has a Schur factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Review: [Linear combinations, span, and basis vectors](https://www.youtube.com/watch?v=k7RM-ot2NWY&index=3&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "\n",
    "See Lecture 24 for proofs of above theorems (and more!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic version of the QR algorithm:\n",
    "\n",
    "    for k=1,2,...\n",
    "        Q, R = A\n",
    "        A = R @ Q\n",
    "        \n",
    "Under suitable assumptions, this algorithm converges to the Schur form of A!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written again, only with subscripts:\n",
    "\n",
    "$A_0 = A$\n",
    "\n",
    "for $k=1,2,\\ldots$\n",
    "\n",
    "   $\\quad Q_k$, $R_k$ = $A_{k-1}$\n",
    "    \n",
    "   $\\quad A_k$ = $R_k Q_k$\n",
    "        \n",
    "We can think of this as constructing sequences of $A_k$, $Q_k$, and $R_k$.\n",
    "\n",
    "$$ A_k = Q_k \\, R_k $$\n",
    "\n",
    "$$ Q_k^{-1} \\, A_k = R_k$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$ R_k Q_k = Q_k^{-1} \\, A_k \\, Q_k $$\n",
    "\n",
    "$$A_k = Q_k^{-1} Q_2^{-1} Q_1^{-1} A Q_1 Q_2 \\dots Q_k$$\n",
    "\n",
    "Trefethen proves the following on page 216-217:\n",
    "\n",
    "$$A^k = Q_1 Q_2 \\dots Q_k R_k R_{k-1}\\dots R_1$$\n",
    "\n",
    "**Key**: The QR algorithm constructs orthonormal bases for successive powers $A^k$.  And remember the close relationship between powers of A and the eigen decomposition.\n",
    "\n",
    "To learn more, read up on *Rayleigh quotients*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n = 6\n",
    "A = np.random.rand(n,n)\n",
    "AT = A @ A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pure_qr(A, max_iter=50000):\n",
    "    Ak = np.copy(A)\n",
    "    n = A.shape[0]\n",
    "    QQ = np.eye(n)\n",
    "    for k in range(max_iter):\n",
    "        Q, R = np.linalg.qr(Ak)\n",
    "        Ak = R @ Q\n",
    "        QQ = QQ @ Q\n",
    "        if k % 100 == 0:\n",
    "            print(Ak)\n",
    "            print(\"\\n\")\n",
    "    return Ak, QQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ak, Q = pure_qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Q is orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.eye(n), Q @ Q.T), np.allclose(np.eye(n), Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really really slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Practical QR (QR with shifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Idea**: Instead of factoring $A_k$ as $Q_k R_k$, \n",
    "\n",
    "1. Get the QR factorization $$A_k - s_k I = Q_k R_k$$\n",
    "2. Set $$A_{k+1} = R_k Q_k + s_k I$$\n",
    "\n",
    "Choose $s_k$ to approximate an eigenvalue of $A$.  We'll use $s_k = A_k(m,m)$. \n",
    "\n",
    "The idea of adding shifts to speed up convergence shows up in many algorithms in numerical linear algebra (including the power method, inverse iteration, and Rayleigh quotient iteration).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework: Add shifts to the QR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Exercise: Add shifts to the QR algorithm\n",
    "#Exercise: def practical_qr(A, iters=10):\n",
    "#Exercise:     return Ak, Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ak, Q = practical_qr(A, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Q is orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.eye(n), Q @ Q.T), np.allclose(np.eye(n), Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: This is better than the unshifted version (which wasn't even guaranteed to converge), but is still really slow!  In fact, it is $\\mathcal{O}(n^4)$, which is awful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of symmetric matrices, it's $\\mathcal{O}(n^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you start with a **Hessenberg matrix** (zeros below the first subdiagonal), it's faster: $\\mathcal{O}(n^3)$, and $\\mathcal{O}(n^2)$ if symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A Two-Phase Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In practice, a two phase approach is used to find eigenvalues:\n",
    "\n",
    "1. Reduce the matrix to *Hessenberg* form (zeros below the first subdiagonal)\n",
    "2. Iterative process that causes Hessenberg to converge to a *triangular* matrix.  The eigenvalues of a triangular matrix are the values on the diagonal, so we are finished!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/nonhermitian_eigen.JPG\" alt=\"2 phase approach\" style=\"width: 80%\"/>\n",
    "(source: Trefethen, Lecture 25)\n",
    "\n",
    "In the case of a Hermitian matrix, this approach is even faster, since the intermediate step is also Hermitian (and a Hermitian Hessenberg is *tridiagonal*).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/hermitian_eigen.JPG\" alt=\"2 phase approach\" style=\"width: 80%\"/>\n",
    "(source: Trefethen, Lecture 25)\n",
    "\n",
    "Phase 1 reaches an exact solution in a finite number of steps, whereas Phase 2 theoretically never reaches the exact solution.\n",
    "\n",
    "We've already done step 2: the QR algorithm.  Remember that it would be possible to just use the QR algorithm, but ridiculously slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Arnoldi Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use the Arnoldi iteration for phase 1 (and the QR algorithm for phase 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 5\n",
    "A0 = np.random.rand(n,n)  #.astype(np.float64)\n",
    "A = A0 @ A0.T\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Linear Algebra Review: Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When vector $\\mathbf{b}$ is projected onto a line $\\mathbf{a}$, its projection $\\mathbf{p}$ is the part of $\\mathbf{b}$ along that line $\\mathbf{a}$.\n",
    "\n",
    "Let's look at interactive graphic (3.4) for [section 3.2.2: Projections](http://immersivemath.com/ila/ch03_dotproduct/ch03.html) of the [Immersive Linear Algebra online book](http://immersivemath.com/ila/index.html).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/projection_line.png\" alt=\"projection\" style=\"width: 70%\"/>\n",
    "(source: [Immersive Math](http://immersivemath.com/ila/ch03_dotproduct/ch03.html))\n",
    "\n",
    "And here is what it looks like to project a vector onto a plane:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/projection.png\" alt=\"projection\" style=\"width: 70%\"/>\n",
    "(source: [The Linear Algebra View of Least-Squares Regression](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b))\n",
    "\n",
    "When vector $\\mathbf{b}$ is projected onto a line $\\mathbf{a}$, its projection $\\mathbf{p}$ is the part of $\\mathbf{b}$ along that line $\\mathbf{a}$.  So $\\mathbf{p}$ is some multiple of $\\mathbf{a}$. Let $\\mathbf{p} = \\hat{x}\\mathbf{a}$ where $\\hat{x}$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The key to projection is orthogonality:** The line *from* $\\mathbf{b}$ to $\\mathbf{p}$ (which can be written $\\mathbf{b} - \\hat{x}\\mathbf{a}$) is perpendicular to $\\mathbf{a}$.\n",
    "\n",
    "This means that $$ \\mathbf{a} \\cdot (\\mathbf{b} -  \\hat{x}\\mathbf{a}) = 0 $$\n",
    "\n",
    "and so $$\\hat{x} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Motivation**:\n",
    "\n",
    "We want orthonormal columns in $Q$ and a Hessenberg $H$ such that $A Q = Q H$.\n",
    "\n",
    "Thinking about it iteratively, $$ A Q_n = Q_{n+1} H_n $$ where $Q_{n+1}$ is $n\\times n+1$ and $H_n$ is $n+1 \\times n$.  This creates a solvable recurrence relation.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/numerical-linear-algebra/694453e105b2a8f96b62b62d44d234c561eba268/nbs/images/arnoldi.jpg\" alt=\"arnoldi\" style=\"width: 95%\"/>\n",
    "(source: Trefethen, Lecture 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Pseudo-code for Arnoldi Algorithm**\n",
    "\n",
    "    Start with an arbitrary vector (normalized to have norm 1) for first col of Q\n",
    "    for n=1,2,3...\n",
    "        v = A @ nth col of Q\n",
    "        for j=1,...n\n",
    "            project v onto q_j, and subtract the projection off of v\n",
    "            want to capture part of v that isn't already spanned by prev columns of Q\n",
    "            store coefficients in H\n",
    "        normalize v, and then make it the (n+1)th column of Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that we are multiplying A by the previous vector in Q and removing the components that are not orthogonal to the existing columns of Q.\n",
    "\n",
    "**Question:** Repeated multiplications of A?  Does this remind you of anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Exercise Answer\n",
    "The *Power Method* involved iterative multiplications by A as well!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### About how the Arnoldi Iteration works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- With the Arnoldi Iteration, we are finding an orthonormal basis for the *Krylov subspace*. \n",
    "The Krylov matrix $$ K = \\left[b \\; Ab \\; A^2b \\; \\dots \\; A^{n-1}b \\right]$$\n",
    "has a QR factorization\n",
    "$$K = QR$$\n",
    "and that is the same $Q$ that is being found in the Arnoldi Iteration.  Note that the Arnoldi Iteration does not explicity calculate $K$ or $R$.\n",
    "\n",
    "- Intuition: K contains good information about the largest eigenvalues of A, and the QR factorization reveals this information by peeling off one approximate eigenvector at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Arnoldi Iteration is two things:\n",
    "1. the basis of many of the iterative algorithms of numerical linear algebra\n",
    "2. a technique for finding eigenvalues of nonhermitian matrices\n",
    "(Trefethen, page 257)\n",
    "\n",
    "**How Arnoldi Locates Eigenvalues**\n",
    "\n",
    "1. Carry out Arnoldi iteration\n",
    "2. Periodically calculate the eigenvalues (called *Arnoldi estimates* or *Ritz values*) of the Hessenberg H, using the QR algorithm\n",
    "3. Check at whether these values are converging.  If they are, they're probably eigenvalues of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Decompose square matrix A @ Q ~= Q @ H\n",
    "def arnoldi(A):\n",
    "    m, n = A.shape\n",
    "    assert(n <= m)\n",
    "    \n",
    "    # Hessenberg matrix\n",
    "    H = np.zeros([n+1,n]) #, dtype=np.float64)\n",
    "    # Orthonormal columns\n",
    "    Q = np.zeros([m,n+1]) #, dtype=np.float64)\n",
    "    # 1st col of Q is a random column with unit norm\n",
    "    b = np.random.rand(m)\n",
    "    Q[:,0] = b / np.linalg.norm(b)\n",
    "    for j in range(n):\n",
    "        v = A @ Q[:,j]\n",
    "        for i in range(j+1):\n",
    "            #This comes from the formula for projection of v onto q.\n",
    "            #Since columns q are orthonormal, q dot q = 1\n",
    "            H[i,j] = np.dot(Q[:,i], v)\n",
    "            v = v - (H[i,j] * Q[:,i])\n",
    "        H[j+1,j] = np.linalg.norm(v)\n",
    "        Q[:,j+1] = v / H[j+1,j]\n",
    "        \n",
    "        # printing this to see convergence, would be slow to use in practice\n",
    "        print(np.linalg.norm(A @ Q[:,:-1] - Q @ H))\n",
    "    return Q[:,:-1], H[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Q, H = arnoldi(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check that H is tri-diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write code to confirm that:\n",
    "1. AQ = QH\n",
    "2. Q is orthonormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Exercise:\n",
    "np.allclose(A @ Q, Q @ H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Exercise:\n",
    "np.allclose(np.eye(len(Q)), Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### General Case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**General Matrix**: Now we can do this on our general matrix A (not symmetric).  In this case, we are getting a Hessenberg instead of a Tri-diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Q0, H0 = arnoldi(A0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check that H is Hessenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(A0 @ Q0, Q0 @ H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(np.eye(len(Q0)), Q0.T @ Q0), np.allclose(np.eye(len(Q0)), Q0 @ Q0.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def eigen(A, max_iter=20):\n",
    "    Q, H = arnoldi(A)\n",
    "    Ak, QQ = practical_qr(H, max_iter)\n",
    "    U = Q @ QQ\n",
    "    D = np.diag(Ak)\n",
    "    return U, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "A0 = np.random.rand(n,n)\n",
    "A = A0 @ A0.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "U, D = eigen(A, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(U @ np.diag(D) @ U.T - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(U @ np.diag(D) @ U.T, A, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's find some eigenvalues!\n",
    "\n",
    "\n",
    "from [Nonsymmetric Eigenvalue Problems](https://sites.math.washington.edu/~morrow/498_13/eigenvalues.pdf) chapter:\n",
    "\n",
    "Note that \"direct\" methods must still iterate, since finding eigenvalues is mathematically equivalent to finding zeros of polynomials, for which no noniterative methods can exist. We call a method direct if experience shows that it (nearly) never fails to converge in a\n",
    "fixed number of iterations.\n",
    "\n",
    "Iterative methods typically provide approximations only to a subset of the eigenvalues and eigenvectors and are usually run only long enough to get a few adequately accurate eigenvalues rather than a large number\n",
    "\n",
    "our ultimate algorithm: the shifted Hessenberg QR algorithm\n",
    "\n",
    "More reading:\n",
    "- [The Symmetric Eigenproblem and SVD](https://sites.math.washington.edu/~morrow/498_13/eigenvalues2.pdf)\n",
    "- [Iterative Methods for Eigenvalue Problems](https://sites.math.washington.edu/~morrow/498_13/eigenvalues3.pdf) Rayleigh-Ritz Method, Lanczos algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Coming Up\n",
    "\n",
    "We will be coding our own QR decomposition (two different ways!) in the future, but first we are going to see another way that the QR decomposition can be used: to calculate linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetric matrices come up naturally:\n",
    "- distance matrices\n",
    "- relationship matrices (Facebook or LinkedIn)\n",
    "- ODEs\n",
    "\n",
    "We will look at positive definite matrices, since that guarantees that all the eigenvalues are real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the confusing language of NLA, the QR algorithm is *direct*, because you are making progress on all columns at once.  In other math/CS language, the QR algorithm is *iterative*, because it iteratively converges and never reaches an exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structured orthogonalization.  In the language of NLA, Arnoldi iteration is considered an *iterative* algorithm, because you could stop part way and have a few columns completed.\n",
    "\n",
    "a Gram-Schmidt style iteration for transforming a matrix to Hessenberg form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "state": {
    "2987aec2ec494d2b9b31bdf93ba87cb6": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "84bffde89e894158860b175e986f4d61": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
